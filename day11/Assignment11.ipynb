{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 11"
      ],
      "metadata": {
        "id": "RRFzLrGlMC7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYFt4WcZMACd",
        "outputId": "40f1f66c-d8f7-4c33-b0d1-8067255bab6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', 'it', 'allows', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'building', 'a', 'tokenizer', 'is', 'one', 'of', 'the', 'first', 'steps', 'in', 'many', 'nlp', 'systems', 'a', 'tokenizer', 'breaks', 'text', 'into', 'smaller', 'pieces', 'called', 'tokens', 'these', 'tokens', 'are', 'then', 'used', 'for', 'further', 'processing', 'such', 'as', 'classification', 'translation', 'or', 'sentiment', 'analysis', 'creating', 'a', 'dynamic', 'vocabulary', 'ensures', 'that', 'every', 'unique', 'word', 'receives', 'a', 'consistent', 'numerical', 'representation']\n",
            "\n",
            "Token IDs:\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 5, 21, 4, 22, 8, 23, 24, 25, 26, 27, 28, 29, 5, 21, 30, 31, 32, 33, 34, 35, 36, 37, 36, 38, 39, 40, 41, 42, 3, 43, 44, 45, 46, 47, 48, 49, 50, 5, 51, 52, 53, 54, 55, 56, 57, 58, 5, 59, 60, 61]\n",
            "\n",
            "Vocabulary:\n",
            "{'natural': 1, 'language': 2, 'processing': 3, 'is': 4, 'a': 5, 'fascinating': 6, 'field': 7, 'of': 8, 'artificial': 9, 'intelligence': 10, 'it': 11, 'allows': 12, 'computers': 13, 'to': 14, 'understand': 15, 'interpret': 16, 'and': 17, 'generate': 18, 'human': 19, 'building': 20, 'tokenizer': 21, 'one': 22, 'the': 23, 'first': 24, 'steps': 25, 'in': 26, 'many': 27, 'nlp': 28, 'systems': 29, 'breaks': 30, 'text': 31, 'into': 32, 'smaller': 33, 'pieces': 34, 'called': 35, 'tokens': 36, 'these': 37, 'are': 38, 'then': 39, 'used': 40, 'for': 41, 'further': 42, 'such': 43, 'as': 44, 'classification': 45, 'translation': 46, 'or': 47, 'sentiment': 48, 'analysis': 49, 'creating': 50, 'dynamic': 51, 'vocabulary': 52, 'ensures': 53, 'that': 54, 'every': 55, 'unique': 56, 'word': 57, 'receives': 58, 'consistent': 59, 'numerical': 60, 'representation': 61}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.next_id = 1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if token not in self.vocab:\n",
        "                self.vocab[token] = self.next_id\n",
        "                self.next_id += 1\n",
        "            token_ids.append(self.vocab[token])\n",
        "\n",
        "        return tokens, token_ids\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.vocab\n",
        "\n",
        "\n",
        "# Larger paragraph example\n",
        "text = \"\"\"\n",
        "Natural language processing is a fascinating field of artificial intelligence.\n",
        "It allows computers to understand, interpret, and generate human language.\n",
        "Building a tokenizer is one of the first steps in many NLP systems.\n",
        "A tokenizer breaks text into smaller pieces called tokens.\n",
        "These tokens are then used for further processing such as classification,\n",
        "translation, or sentiment analysis. Creating a dynamic vocabulary ensures\n",
        "that every unique word receives a consistent numerical representation.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokens, token_ids = tokenizer.tokenize(text)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\nToken IDs:\")\n",
        "print(token_ids)\n",
        "\n",
        "print(\"\\nVocabulary:\")\n",
        "print(tokenizer.get_vocab())"
      ]
    }
  ]
}